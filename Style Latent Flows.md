# Exploiting Style Latent Flows for Generalizing Deepfake Video Detection

## Q0：Motivation

![image-20241010124409786](https://raw.githubusercontent.com/bobo0214/typora_pic/main/img/202410101244980.png)



## Q1：deepfake 检测框架

![image-20241009200845753](https://raw.githubusercontent.com/bobo0214/typora_pic/main/img/202410092008882.png)

### 1. StyleGRU module

+ **输入**：

  + $I_p$：原视频片段 $I$ 经过FFHQ预处理得到的由cropped face组成的视频片段，调整大小为$R^{256×256×l}$（ $l$ 是原视频片段的连续帧数）

+ **Style latent vectors的提取**：

  + 使用 **pSp 编码器**从视频帧中提取风格潜向量
    + pSp 编码器是一种专门针对 StyleGAN 潜空间设计的编码器，它能够将图像编码为风格潜向量，并保留图像的视觉特征。
  +  $  \mathbf {S} = pSp(I_{p}) \equiv \{s_{1}, s_{2}, ..., s_{l}\}$
    + $s_{1}, s_{2}, ..., s_{l}$ ：对应每帧提起的风格潜向量

+ **Style flow的计算**:

  + 将连续帧的<u>风格潜向量</u>进行差分，得到**风格流** 
    + 风格流反映了风格潜变量在时间上的变化，包含了视频的动态信息，例如面部表情和几何变换的变化
  + 风格流 $   \Delta \mathbf {S} = \{\Delta s_{1}, \Delta s_{2}, ..., \Delta s_{(l-1)}\}$
    + 其中 $∆s_i = s_{i+1} − s_i$

+ **GRU编码**

  + 将风格流 $\Delta \mathbf {S}$ 作为<font color='pink'>GRU</font>的输入
    + GRU 是一种循环神经网络，能够捕捉序列数据的时序特征。
    + 通过 GRU 的循环结构，可以有效地学习风格流的动态变化。
  + GRU 的最后一个隐藏状态包含了对风格流动态变化的完整编码，作为**风格时间特征 $E_{style}$**
    + 通过pSp编码器提取的风格特征向量是GAN反演任务的特征，因此不足以直接应用于深度伪造检测任务
    + $E_{style}$ 是一个向量，**包含了风格潜变量在时间上的变化信息**，可以作为深度伪造检测的依据
  +  $ E_{\text {style}} = \mathbf {GRU}(\Delta \mathbf {S})$

  

### 2. 3D ResNet-50 module

+ **输入**：

  + $I$：原视频片段，调整大小为$R^{224×224×l}$

+ **提取视频内容特征 $C_{content}$**

  +  $C_{content}$：组合了视频帧的视觉和时序特征
  + $C_{\text {content}} = \mathbf {3D~CNN}(I))$

  

### 3. Style attention module（风格注意力）

+ **输入**：

  + 将 StyleGRU 生成的风格时间特征 $E_{style}$ 和 3D ResNet-50 生成的内容特征 $C_{content}$ 输入到 SAM 中

+ **<font color='pink'>注意力机制</font>**：

  + 使用**线性投影函数**将 $E_{style}$ 和 $C_{content}$ 转换为键和值、查询
    + 为什么要多余使用线性投影函数，不直接计算注意力分数呢？
      + 注意力机制需要查询、键和值具有相同的维度。如果直接计算注意力分数，可能无法有效地处理维度不匹配的问题。
      + 直接计算注意力分数可能无法有效地提取$E_{style}$ 和$C_{content}$ 中的关键信息 和 映射关系。
    + 为什么将content feature 作为查询？
      + these features hold the potential to identify anomalies in the style flow, driven by their encoded temporal inconsistency.
    + 公式
      +  $Q_{C} = \mathbf {\phi }_{q}(C_\text {content}),~ K_{E} = \mathbf {\phi }_{k}(E_\text {style}),~ V_{E} = \mathbf {\phi }_{v}(E_\text {style})$
      + 使用全连接层的线性投影函数
  + 使用注意力机制**计算注意力分数**，例如**点积**或余弦相似度
    + $ \mathbf {SA}(Q_{C}, K_{E}) = s\left (\frac {Q_{C}K_{E}}{\sqrt {d_{K_{E}}}}\right )$
      + $s()$：softmax函数
  + 使用注意力分数加权组合 $E_{style}$ 和 $C_{content}$，得到加权后的风格时间特征 E_style'
    +  $ \mathbf {SAM}(E, C) = \phi \big (\mathbf {SA}(Q_{C}, K_{E}) \cdot V_{E}\big )$
      + $\phi()$：SAM的最后一个线性投影层

+ **输出**：

  + 将 E_style' 输入到后续的深度伪造检测模块

  

### 4. Temporal transformer encoder module （时间transformer编码器模块 TTE）

+ 模型
  + **Transformer 编码器**
    + Transformer 编码器是一种基于自注意力机制的神经网络
    + Transformer 编码器由多个编码器层组成，每个编码器层包含：
      - **自注意力模块**：计算序列中不同元素之间的注意力分数，并根据注意力分数加权组合特征。
      - **前馈神经网络**：对加权组合后的特征进行非线性变换，从而增加模型的表示能力。
  + **多头注意力**：
    - TTE 模块使用多头注意力机制，将输入特征分成多个“头”，每个“头”独立地进行自注意力计算。
    - 多头注意力机制能够提高模型的并行度和鲁棒性，并捕捉序列数据中不同方面的特征。
  + **分类头**：
    - TTE 模块的输出是一个特征向量，该特征向量包含了视频帧的时空特征。
    - 分类头将特征向量映射到二进制类别（真实/伪造），并输出一个概率值，表示输入视频为真实或伪造的可能性。

+ 工作流程

  + **输入**：

    + 将SAM的输出线性投影到16个时间步长，作为输入

  + **公式**：

    + $\hat {y} = \sigma \big (f_{cls}(\mathbf {TTE}(C_{\text {content}} + \mathbf {SAM}(E_{\text {style}}, C_{\text {content}})))\big )$
    + $f_{cls}$：一个全连接层的函数用来表示分类头
    + $\sigma$：sigmoid函数

  + **输出**：

    + TTE 输出经过 sigmoid 函数处理的概率值（即后续分类头的输入），表示输入视频为真实或伪造。

    

## Q2：训练过程

![image-20241010112302616](https://raw.githubusercontent.com/bobo0214/typora_pic/main/img/202410101123866.png)

### Stage 1: style representation learning

主要目的是训练 StyleGRU 模块，使其能够学习风格流的有效表示，从而区分真实视频和伪造视频。

采用**监督对比学习**来产生风格流的有效表示。

- **数据准备**：
  - 从训练数据集中提取视频帧，并使用 pSp 编码器提取风格潜向量。
  - 将连续帧的风格潜向量进行差分，得到风格流 (style flow)。
  - 将风格流组织成批次，并分为**锚点、正样本和负样本**。
    - 锚点
      - 通过滑动窗口策略从训练数据集中随机选择的风格流样本$∆Sa$
      - 锚点样本的标签**可以是真实视频或伪造视频**

    - 正样本
      - 通过随机抽样策略从**与锚点样本标签相同**的视频中随机选择的风格流样本$∆Sp$

    - 负样本
      - 选择策略同上二选一
      - **与锚点样本标签不同**的视频中选择风格流样本$∆Sn$

- **损失**
  - **三元组损失 $L_{tri}$训练**(triplet loss)：
    - 目的：学习风格流的有效表示
    - 三元组损失计算锚点、正样本和负样本之间的特征距离，并鼓励锚点和正样本的特征距离更近，而锚点和负样本的特征距离更远。
    - $ L_{tri} = \mathbf {max}\big (||E_{a} - E_{p}||_{2}^{2}- ||E_{a} - E_{n}||_{2}^{2} + \alpha , 0\big )$
    - $E_{a}$ $E_{p}$ $E_{n}$ 分别是对应风格流产生的风格时间特征 

  - **分类损失 $L_{cls}$训练**(classification loss)：
    - 目的：能够区分真实视频和伪造视频。
    - 分类损失使用二进制交叉熵（BCE）损失来计算预测标签和真实标签之间的差异，并鼓励模型正确地分类视频。

  - **$ L = L_{tri} + \lambda L_{cls}$     ($\lambda$是超参数)**

- **优化器**：
  - 使用 Adam 优化器进行训练，并设置学习率、批大小等超参数。
- **输出**
  - Style representation learning 阶段训练完成后，得到一个训练好的 StyleGRU 模块，它能够学习风格流的有效表示，并区分真实视频和伪造视频。




### Stage 2: style attention-based deepfake detection

+ **目的**：提高深度伪造视频检测的准确率和鲁棒性

+ **特征提取**：
  - 使用训练好的 StyleGRU 模块从视频帧中提取风格时间特征 E_style
  - 使用 3D ResNet-50 模块从视频帧中提取内容特征 C_content
+ **特征融合**：
  - 使用 SAM 将风格时间特征 E*style 和内容特征 C*content 相结合。
  - SAM 使用注意力机制来动态地调整特征权重，使得模型更加关注重要的信息。
+ **特征编码**：
  - 将 SAM 输出的特征向量输入到 TTE 模块。
  - TTE 模块使用 Transformer 编码器对特征向量进行编码，并提取视频帧的时空特征。
+  **分类**：
  - 将 TTE 模块的输出输入到分类头，进行深度伪造视频的检测。
  - 分类头输出一个概率值，表示输入视频为真实或伪造的可能性。
+ **优化器**：
  - 使用 SGD 优化器进行训练，并设置学习率、批大小等超参数。
+  **输出**：
  - Stage 2 的输出是一个概率值，表示输入视频为真实或伪造的可能性。
  - 该概率值可以用于判断输入视频是否为深度伪造视频。



## 补充

### 1. Disentangled latent space

> 解耦的潜在空间：这指的是GAN模型（特别是StyleGAN）能够生成一个结构良好的潜在空间。在这个潜在空间中，图像的不同特征（如姿势、发型、表情等）可以**独立**地控制和调整。

### 2. GAN inversion

> **GAN反演**是一个技术流程，其目标是找到一个潜在代码（latent code），这个代码能够在GAN的潜在空间中生成与目标图像非常相似的图像。简单来说，就是通过逆向操作找到可以生成给定图像的潜在表示。
>
> ### 1. 预训练的GAN模型
>
> 首先，需要一个预训练好的GAN模型，比如StyleGAN或BigGAN。这些模型已经学习到从潜在空间生成高质量图像的映射关系。
>
> ### 2. 潜在空间
>
> GAN的潜在空间通常是一个高维空间，输入到生成器的潜在向量（如 $z$ 或 $w$ 向量）能够控制生成图像的特征。
>
> ### 3. 图像输入
>
> 选择一张需要反演的真实图像$I$。我们的目标是找到一个潜在向量 $z^*$，使得生成器能够生成与图像 $I$ 尽可能相似的图像 $G(z^*)$。
>
> ### 4. 优化过程
>
> 使用优化算法（如梯度下降）来迭代地调整潜在向量 $z$。具体步骤如下：
>
> - **损失函数**：定义一个损失函数来衡量生成图像与输入图像之间的相似度，通常使用L2损失或感知损失（perceptual loss）。
>
>   $L(z)= \| G(z) - I \|^2$
>
> - **反向传播**：通过计算损失函数的梯度，更新潜在向量 $z$。
>
> ### 5. 迭代优化
>
> 重复步骤4，直到损失函数收敛，找到使生成图像与输入图像相似的最佳潜在向量 $z^*$。
>
> ### 6. 输出结果
>
> 完成优化后，得到的潜在向量 $z^*$ 可以用来分析或修改图像属性，或者用于其他应用，如图像编辑、风格迁移等。



> **编码器encoders**在GAN反演中的作用：
>
> 1. **图像到潜在空间的映射**：
>
>    - 编码器的主要功能是将输入的真实图像 $I$ 转换为潜在向量$z$。这一过程也被称为“编码”或“反演”。
>
>    - 通过学习特征表示，编码器能够有效捕捉图像的关键信息，并将其压缩成潜在空间中的向量。
>
> 2. **快速估计**：
>    - 编码器可以快速地为任意图像提供一个潜在向量，减少了对潜在向量的直接优化过程。这样可以加快反演的速度。
>
> 3. **学习到的映射**：
>
>    - 在某些情况下，编码器是**经过训练的模型**（例如，Autoencoder或基于GAN的编码器），它学习了如何将输入映射到潜在空间。
>
>    
>
> **解码器decoders**在GAN反演中的作用：
>
> 1. **潜在向量到图像的生成**：
>
>    - 解码器的作用是从潜在向量 $z$ 生成图像 $G(z)$。它通常是GAN生成器的部分。
>
>    - 解码器通过将潜在向量转换为高维图像，确保生成的图像在视觉上与输入图像相似。
>
> 2. **重构图像**：
>    - 在反演过程中，解码器用于将编码器生成的潜在向量转换为图像。这一过程是通过优化潜在向量来不断调整生成的图像，使其与原始图像更相似。



### 3. GRU

> GRU（Gated Recurrent Unit）是一种循环神经网络（RNN）的变体，它通过引入**门控机制**来提高 RNN 的效率和性能。
>
> **1. 循环神经网络（RNN）**：
>
> - RNN 是一种神经网络，用于处理序列数据，例如时间序列、文本数据等。
> - RNN 的基本思想是将当前输入与之前的信息结合起来，生成当前的输出。
>
> **2. 门控机制**：
>
> - GRU 通过门控机制来控制信息在 RNN 中的流动。
> - 门控机制包括：
>   - **重置门（Reset Gate）**：控制当前输入信息对 RNN 状态的影响。
>     - 根据当前输入和上一个 RNN 状态，决定**是否**更新 RNN 状态。
>   - **更新门（Update Gate）**：控制 RNN 状态的更新。
>     - 根据当前输入和上一个 RNN 状态，决定**如何**更新 RNN 状态。
>   - **输出门（Output Gate）**：控制 RNN 输出的信息。
>     - 根据当前输入、上一个 RNN 状态和更新后的 RNN 状态，决定如何生成当前输出。
>
> **3. 优点**：
>
> - 与传统的 RNN 相比，GRU 具有以下优点：
>   - **效率更高**：GRU 的结构比 RNN 简单，计算量更小，训练速度更快。
>   - **更易于训练**：GRU 的门控机制可以有效地控制信息流动，使得 RNN 更容易训练。
>   - **更适用于长序列**：GRU 能够更好地捕捉长序列中的长期依赖关系。
>



### 4. 3D ResNet-50

> **1. 模型结构**：
>
> - 3D ResNet-50 模型是基于 2D ResNet-50 模型扩展而来的。
> - 2D ResNet-50 模型是一个经典的图像分类模型，它由多个残差块组成，每个残差块包含多个卷积层和池化层。
> - 3D ResNet-50 模型将 2D ResNet-50 模型的卷积层扩展为 3D 卷积层，使得模型能够处理视频帧的三维数据（时间维度、空间维度）。
>
> **2. 3D 卷积层**：
>
> - 3D 卷积层与 2D 卷积层类似，但它能够同时处理时间维度和空间维度上的信息。
> - 3D 卷积层可以提取视频帧的时空特征，例如：
>   - **时间特征**：例如运动轨迹、速度、加速度等。
>   - **空间特征**：例如纹理、形状、颜色等。
>
> **3. 残差块**：
>
> - 3D ResNet-50 模型包含多个残差块，每个残差块包含多个 3D 卷积层和激活函数。
> - 残差块的结构与 2D ResNet-50 模型类似，但它使用 3D 卷积层和 3D 池化层。
> - 残差块能够有效地学习视频帧的复杂特征，并通过残差连接避免梯度消失问题。
>
> **4. 全局平均池化**：
>
> - 在 3D ResNet-50 模型的最后一个残差块之后，使用全局平均池化将特征图压缩为一个特征向量。
> - 全局平均池化能够有效地提取视频帧的特征，并减少模型参数量。
>
> **5. 内容特征**：
>
> - 3D ResNet-50 模型的输出是一个特征向量，包含了视频帧的视觉和时序特征，称为内容特征 C_content。
> - 内容特征 C_content 可以用于深度伪造检测，例如：
>   - **检测视觉伪影**：例如颜色失真、纹理不一致等。
>   - **检测时序伪影**：例如运动不自然、时间跳跃等。



### 5. 注意力机制

> 注意力机制（Attention Mechanism）是一种灵活且强大的技术，最早应用于自然语言处理（NLP），后来扩展到计算机视觉和其他领域。它的核心思想是让模型能够动态地**关注输入数据的不同部分**，而不仅仅是处理全部输入的平均信息。这样，模型可以根据任务的需求，分配更多的“注意力”给重要的部分，从而提高性能。
>
> ### 核心思想
>
> 注意力机制的基本目的是让模型从输入序列中**选择性地聚焦**在最相关的部分。与传统模型处理每个输入数据点的方式不同，注意力机制允许模型根据上下文信息**动态调整每个输入的权重**。
>
> ### 工作原理
>
> 1. **输入表示**：假设有一组输入 $X = \{x_1, x_2, ..., x_n\}$，模型需要从这些输入中提取信息。
>
> 2. **打分函数**：为了确定输入中每个元素的重要性，首先要计算一个打分函数，衡量输入的各部分与当前任务的相关性。通常这个打分函数会输出一个“注意力权重”，表示每个输入的重要程度。常见的打分函数包括点积（dot-product）、双线性函数（bilinear）、MLP等。
>
>    $score(x_i) = f(x_i, query)$
>
>    其中，$query$ 是用于查询输入的特定向量，$f$ 是打分函数。
>
> 3. **权重计算（Softmax）**：打分函数的结果通过一个 softmax 函数，转化为一组概率权重，使其总和为 1。
>
>    $ai=\frac{\exp(score(x_i))}{\sum_j \exp(score(x_j))}$
>
> 4. **加权求和**：使用注意力权重 $a_i$ 对输入进行加权求和，以得到最终的输出。
>
>    $output=\sum_i a_i \cdot x_i$
>
> 这种加权求和方式让模型能够根据任务需要聚焦于最相关的输入，同时忽略不重要的信息。
>
> ### 注意力机制的类型
>
> 1. **Soft Attention（软注意力）**：
>    - Soft Attention 是大多数注意力机制的基础，通过<u>对输入进行加权求和</u>，权重通过 softmax 计算得出。
> 2. **Self-Attention（自注意力）**：
>    - 自注意力机制（Self-Attention）是Transformer架构的核心。它的特点是<u>每个输入元素与其他所有输入元素之间</u>计算注意力权重。这种机制可以捕捉输入序列中长距离的依赖关系，特别适用于自然语言处理中的文本序列或计算机视觉中的图像分割。
> 3. **Hard Attention（硬注意力）**：
>    - 与软注意力不同，硬注意力<u>选择输入的子集，并忽略其他部分</u>。这是一种更具选择性的注意力机制，但由于选择过程不连续，难以通过标准反向传播训练。
>
> ### 应用场景
>
> 1. **自然语言处理（NLP）**：
>    - **机器翻译**：在翻译时，模型可以根据当前词汇，重点关注源句子中的某些词汇，而不是对所有词汇一视同仁。
>    - **文本生成**：生成每个词时，模型可以动态调整对输入序列中各个部分的注意力。
> 2. **计算机视觉（CV）**：
>    - **图像分类**：在对图像进行分类时，模型可以重点关注图像的关键区域，忽略背景信息。
>    - **图像生成**：注意力机制帮助生成模型更好地理解图像局部区域的细节，提升生成图像的质量。
> 3. **语音识别和生成**：模型可以在语音生成或识别的过程中动态调整注意力，关注不同时间片的音频信息。

>在注意力机制中，查询（Query）、键（Key）和值（Value）是三个重要的概念，通常用来描述如何计算注意力权重。以下是它们的含义和作用：
>
>### 查询（Query）
>
>- **定义**：查询是指当前模型在处理特定输入时，用来寻找相关信息的向量。可以理解为“问题”或“请求”。
>- **作用**：查询用于**评估输入数据中哪些部分与当前上下文最相关**，从而决定将多少注意力分配给这些部分。
>
>### 键（Key）
>
>- **定义**：键是与输入数据中每个元素关联的向量，代表了这些元素的特征。可以理解为“索引”或“标识符”。
>- **作用**：每个输入元素都有一个**对应的键**，**查询通过与这些键进行比较**来计算注意力权重。
>
>### 值（Value）
>
>- **定义**：值是与输入数据中每个元素关联的向量，通常用于输出结果。可以理解为“内容”或“信息”。
>- **作用**：最终的注意力输出是通过**加权求和这些值**来得到的，权重由查询和键之间的相似度决定。
>
>### 工作流程
>
>1. **计算注意力权重**：
>
>   - 查询向量与所有键向量进行比较，通常通过点积计算相似度得分。
>
>   + $score(q, k) = q \cdot k$
>
>2. **生成权重**：
>
>   - 将相似度得分通过 softmax 函数转换为概率权重，表示当前查询对每个键的关注程度。
>
>3. **加权求和**：
>
>   - 使用这些权重对所有值进行加权求和，得到最终的注意力输出。
>
>   $output=\sum_i \text{softmax}(score(q, k_i)) \cdot v_i$



### 6. Adam优化器

> Adam优化器（Adaptive Moment Estimation）是一种常用的自适应学习率优化算法，广泛应用于深度学习中。它结合了动量（Momentum）和自适应学习率的优点，旨在加速收敛并提高模型性能。
>
> ### 主要特点
>
> 1. **自适应学习率**：
>
>    - Adam为每个参数维护一个独立的学习率，根据过去的梯度信息动态调整。
>
> 2. **动量项**：
>
>    - 利用过去梯度的指数移动平均，平滑梯度更新，帮助减小震荡，加速收敛。
>
> 3. **公式**： Adam的更新公式主要包括以下步骤：
>
>    - 初始化：
>
>      ​				$m_0 = 0, \quad v_0 = 0, \quad t = 0$
>
>    - 在每个时间步 $t$ 进行更新：
>
>      1. 计算当前梯度 $g_t$。
>
>      2. 更新动量和方差： $m_t= \beta_1 m_{t-1} + (1 - \beta_1) g_t$ 
>
>         ​				      $v_t = \beta_2 v_{t-1} + (1 - \beta_2) g_t^2$
>
>      3. 进行偏差校正： $m^t=\frac{m_t}{1 - \beta_1^t}$,  $ \quad \hat{v}_t = \frac{v_t}{1 - \beta_2^t}$
>
>      4. 更新参数： $θt=\theta_{t-1} - \frac{\alpha}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t$
>
>    其中，$α$ 是学习率，$β_1$ 和 $β_2$ 是控制动量和方差衰减的超参数（通常取值为0.9和0.999），$ϵ$ 是防止除零的微小值（通常取 $10^{-8}$）。
>
> ### 优势
>
> 1. **高效性**：Adam可以快速收敛，尤其是在大规模数据集和高维参数空间中。
> 2. **鲁棒性**：适用于各种任务，包括稀疏梯度和非平稳目标。
> 3. **易于使用**：几乎不需要调整超参数，默认参数通常表现良好。
>
> ### 应用场景
>
> Adam优化器广泛应用于深度学习的各种模型训练，如卷积神经网络（CNN）、循环神经网络（RNN）等，是很多深度学习框架中的默认优化器。它在图像处理、自然语言处理和其他领域的任务中都表现出色。